"""
Speech-to-Text (STT) ëª¨ë“ˆ

Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ê¸´ ì˜¤ë””ì˜¤ íŒŒì¼ì€ 30ì´ˆ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    from data_processing.stt import transcribe_folder
    
    df = transcribe_folder(
        folder_path="audio_files",
        output_path="transcriptions.csv"
    )
"""

import os
import numpy as np
import pandas as pd
import torch
from pathlib import Path
from tqdm import tqdm
from pydub import AudioSegment
from transformers import WhisperProcessor, WhisperForConditionalGeneration


# ìƒìˆ˜ ì„¤ì •
MAX_DURATION_SEC = 30  # WhisperëŠ” 30ì´ˆ(=480000 samples) ê¸°ì¤€
SAMPLE_RATE = 16000
CHUNK_SIZE = MAX_DURATION_SEC * SAMPLE_RATE


def load_audio(path: str, target_sr: int = 16000) -> torch.Tensor:
    """
    ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
    
    Args:
        path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        target_sr: ëª©í‘œ ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 16000)
        
    Returns:
        ì˜¤ë””ì˜¤ ì›¨ì´ë¸Œí¼ í…ì„œ
    """
    ext = os.path.splitext(path)[1][1:]
    audio = AudioSegment.from_file(path, format=ext)
    audio = audio.set_channels(1).set_frame_rate(target_sr)
    samples = np.array(audio.get_array_of_samples()).astype(np.float32) / 32768.0
    waveform = torch.tensor(samples)
    return waveform


def transcribe_long_audio(
    path: str,
    processor: WhisperProcessor,
    model: WhisperForConditionalGeneration,
    device: str,
    sample_rate: int = 16000,
    chunk_size: int = CHUNK_SIZE
) -> str:
    """
    ê¸´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ STT ìˆ˜í–‰
    
    Args:
        path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        processor: Whisper í”„ë¡œì„¸ì„œ
        model: Whisper ëª¨ë¸
        device: ë””ë°”ì´ìŠ¤ (cuda/cpu)
        sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 16000)
        chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’: 30ì´ˆ)
        
    Returns:
        ì „ì²´ ì „ì‚¬ í…ìŠ¤íŠ¸
    """
    audio = load_audio(path, sample_rate)
    total_len = len(audio)
    results = []
    
    num_chunks = (total_len + chunk_size - 1) // chunk_size
    
    for i in tqdm(range(0, total_len, chunk_size), total=num_chunks, desc=f"ğŸ§  STT: {os.path.basename(path)}"):
        chunk = audio[i:i+chunk_size]
        input_features = processor(chunk, sampling_rate=sample_rate, return_tensors="pt").input_features.to(device)
        
        outputs = model.generate(
            input_features,
            num_beams=1,
            num_return_sequences=1,
            output_scores=False,
            return_dict_in_generate=True,
            early_stopping=False,  # num_beams=1ì¼ ë•Œ early_stoppingì€ ì˜ë¯¸ ì—†ìŒ
        )
        
        decoded = processor.batch_decode(outputs.sequences, skip_special_tokens=True)
        results.append(decoded[0].strip())
    
    return " ".join(results)


def transcribe_folder(
    folder_path: str,
    output_path: str = "transcriptions.csv",
    model_name: str = "openai/whisper-large-v3",
    device: str = None,
    file_extensions: list = None
) -> pd.DataFrame:
    """
    í´ë” ë‚´ ìŒì„± íŒŒì¼ë“¤ì„ STT ì²˜ë¦¬í•˜ì—¬ CSV íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        folder_path: ì…ë ¥ í´ë” ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        output_path: ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        model_name: Whisper ëª¨ë¸ëª… (ê¸°ë³¸ê°’: "openai/whisper-large-v3")
        device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        file_extensions: ì²˜ë¦¬í•  íŒŒì¼ í™•ì¥ì ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: [".mp4", ".wav", ".mp3", ".m4a"])
        
    Returns:
        ì „ì‚¬ ê²°ê³¼ DataFrame
    """
    base_path = Path(__file__).parent.parent
    folder_full_path = base_path / folder_path
    output_full_path = base_path / output_path
    
    if not folder_full_path.exists():
        raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_full_path}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # íŒŒì¼ í™•ì¥ì ì„¤ì •
    if file_extensions is None:
        file_extensions = [".mp4", ".wav", ".mp3", ".m4a", ".flac", ".ogg"]
    
    print(f"ğŸ“‚ í´ë”: {folder_full_path}")
    print(f"ğŸ“± ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ¤– ëª¨ë¸: {model_name}")
    
    # ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¥ Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
    processor = WhisperProcessor.from_pretrained(model_name)
    model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # íŒŒì¼ ëª©ë¡ ìƒì„±
    file_list = sorted([
        f for f in os.listdir(folder_full_path)
        if any(f.lower().endswith(ext.lower()) for ext in file_extensions)
    ])
    
    if not file_list:
        print(f"âš ï¸ ê²½ê³ : {folder_full_path}ì— ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame(columns=["index", "filename", "transcription", "text_length"])
    
    print(f"ğŸ“Š ì²˜ë¦¬í•  íŒŒì¼: {len(file_list)}ê°œ")
    
    # STT ì²˜ë¦¬
    data = []
    for idx, file in enumerate(file_list):
        full_path = folder_full_path / file
        try:
            transcription = transcribe_long_audio(
                str(full_path),
                processor,
                model,
                device
            )
            data.append({
                "index": idx,
                "filename": file,
                "transcription": transcription,
                "text_length": len(transcription)
            })
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ - {file}: {e}")
            data.append({
                "index": idx,
                "filename": file,
                "transcription": "",
                "text_length": 0
            })
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(data)
    
    # CSV ì €ì¥
    output_full_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_full_path, index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_full_path} ({len(df)}ê°œ íŒŒì¼)")
    
    return df


def transcribe_single_file(
    file_path: str,
    model_name: str = "openai/whisper-large-v3",
    device: str = None
) -> str:
    """
    ë‹¨ì¼ ìŒì„± íŒŒì¼ì„ STT ì²˜ë¦¬
    
    Args:
        file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        model_name: Whisper ëª¨ë¸ëª… (ê¸°ë³¸ê°’: "openai/whisper-large-v3")
        device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        
    Returns:
        ì „ì‚¬ í…ìŠ¤íŠ¸
    """
    base_path = Path(__file__).parent.parent
    file_full_path = base_path / file_path
    
    if not file_full_path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_full_path}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ“¥ Whisper ëª¨ë¸ ë¡œë”© ì¤‘... ({model_name})")
    processor = WhisperProcessor.from_pretrained(model_name)
    model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # STT ì²˜ë¦¬
    print(f"ğŸ¤ ì „ì‚¬ ì¤‘: {file_full_path}")
    transcription = transcribe_long_audio(
        str(file_full_path),
        processor,
        model,
        device
    )
    
    print(f"âœ… ì „ì‚¬ ì™„ë£Œ: {len(transcription)}ì")
    return transcription


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (STT)")
    parser.add_argument("--folder", type=str, default=None, help="ì²˜ë¦¬í•  í´ë” ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--file", type=str, default=None, help="ì²˜ë¦¬í•  ë‹¨ì¼ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--output", type=str, default="fss_dataset.csv", help="ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ, í´ë” ì²˜ë¦¬ ì‹œë§Œ ì‚¬ìš©)")
    parser.add_argument("--model", type=str, default="openai/whisper-large-v3", help="Whisper ëª¨ë¸ëª…")
    parser.add_argument("--device", type=str, default=None, help="ë””ë°”ì´ìŠ¤ (cuda/cpu, Noneì´ë©´ ìë™ ì„ íƒ)")
    parser.add_argument("--extensions", type=str, nargs="+", default=[".mp4", ".wav", ".mp3", ".m4a"], help="ì²˜ë¦¬í•  íŒŒì¼ í™•ì¥ì")
    
    args = parser.parse_args()
    
    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
    if args.file:
        transcription = transcribe_single_file(
            file_path=args.file,
            model_name=args.model,
            device=args.device
        )
        print("\n" + "="*50)
        print("ì „ì‚¬ ê²°ê³¼:")
        print("="*50)
        print(transcription)
        return
    
    # í´ë” ì²˜ë¦¬
    if args.folder:
        df = transcribe_folder(
            folder_path=args.folder,
            output_path=args.output,
            model_name=args.model,
            device=args.device,
            file_extensions=args.extensions
        )
        print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ íŒŒì¼")
        print(f"ğŸ“Š ì „ì‚¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        print(df.head())
        return
    
    # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ì—ëŸ¬
    parser.error("--folder ë˜ëŠ” --file ì¤‘ í•˜ë‚˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

