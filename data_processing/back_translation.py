"""
Back Translation (ì—­ë²ˆì—­) ëª¨ë“ˆ

í…ìŠ¤íŠ¸ë¥¼ ì—­ë²ˆì—­í•˜ì—¬ ë°ì´í„° ì¦ê°•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
í•œêµ­ì–´ â†’ ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ ìˆœì„œë¡œ ë²ˆì—­í•˜ì—¬ ì›ë³¸ê³¼ ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    from data_processing.back_translation import back_translate_data
    
    augmented_data = back_translate_data(
        input_csv_path="dataset/train.csv",
        output_csv_path="dataset/train_augmented.csv",
        api_key="your-api-key"
    )
"""

import pandas as pd
import re
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from sentence_transformers import SentenceTransformer, util
from pathlib import Path


class BackTranslator:
    """
    ì—­ë²ˆì—­ ìˆ˜í–‰ í´ë˜ìŠ¤
    
    í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ â†’ ì¼ë³¸ì–´ â†’ í•œêµ­ì–´ë¡œ ì™•ë³µ ë²ˆì—­í•˜ì—¬ ë°ì´í„° ì¦ê°•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, api_key: str, max_workers: int = 5):
        """
        Args:
            api_key: OpenAI API í‚¤
            max_workers: ë™ì‹œ ë²ˆì—­ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: 5)
        """
        self.client = OpenAI(api_key=api_key)
        self.max_workers = max_workers
        self.model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    
    def translate_round_trip(self, text: str) -> tuple[str | None, str | None]:
        """
        ì™•ë³µ ë²ˆì—­: í•œêµ­ì–´ â†’ ì¼ë³¸ì–´ â†’ í•œêµ­ì–´
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸ (í•œêµ­ì–´)
            
        Returns:
            (ì¼ë³¸ì–´_ë²ˆì—­ë³¸, ì—­ë²ˆì—­ëœ_í•œêµ­ì–´_í…ìŠ¤íŠ¸) íŠœí”Œ
        """
        if not isinstance(text, str) or not text:
            return None, None

        try:
            # 1ë‹¨ê³„: í•œêµ­ì–´ â†’ ì¼ë³¸ì–´
            japanese_response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a translator. Translate the given Korean text into Japanese. Always provide the translation without any other explanations or refusals."},
                    {"role": "user", "content": text}
                ],
                timeout=60
            )
            japanese_text = japanese_response.choices[0].message.content.strip()

            # 2ë‹¨ê³„: ì¼ë³¸ì–´ â†’ í•œêµ­ì–´
            korean_response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a translator. Translate the given Japanese text into Korean. Always provide the translation without any other explanations or refusals."},
                    {"role": "user", "content": japanese_text}
                ],
                timeout=60
            )
            retranslated_text = korean_response.choices[0].message.content.strip()

            return japanese_text, retranslated_text

        except Exception as e:
            print(f"âš ï¸ ë²ˆì—­ ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text[:20]}...'): {e}")
            return None, None
    
    def translate_batch(self, texts: list[str]) -> tuple[list[str | None], list[str | None]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì—­ë²ˆì—­
        
        Args:
            texts: ë²ˆì—­í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            (ì¼ë³¸ì–´_ë²ˆì—­ë³¸_ë¦¬ìŠ¤íŠ¸, ì—­ë²ˆì—­ëœ_í•œêµ­ì–´_ë¦¬ìŠ¤íŠ¸) íŠœí”Œ
        """
        japanese_results = [None] * len(texts)
        korean_results = [None] * len(texts)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_index = {
                executor.submit(self.translate_round_trip, text): i
                for i, text in enumerate(texts)
            }

            for future in tqdm(as_completed(future_to_index), total=len(texts), desc="ì—­ë²ˆì—­ ì§„í–‰ ì¤‘"):
                idx = future_to_index[future]
                try:
                    japanese_text, korean_text = future.result()
                    japanese_results[idx] = japanese_text
                    korean_results[idx] = korean_text
                except Exception as e:
                    print(f"âš ï¸ ì¸ë±ìŠ¤ {idx} ë²ˆì—­ ì‹¤íŒ¨: {e}")

        return japanese_results, korean_results
    
    def calculate_similarity(self, original_texts: list[str], retranslated_texts: list[str]) -> list[float]:
        """
        ì›ë³¸ê³¼ ì—­ë²ˆì—­ í…ìŠ¤íŠ¸ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            original_texts: ì›ë³¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            retranslated_texts: ì—­ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¦¬ìŠ¤íŠ¸ (0.0 ~ 1.0)
        """
        # Noneì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜
        originals = [text if text else '' for text in original_texts]
        retranslated = [text if text else '' for text in retranslated_texts]

        # ì„ë² ë”© ìƒì„±
        emb_original = self.model.encode(originals, convert_to_tensor=True)
        emb_retranslated = self.model.encode(retranslated, convert_to_tensor=True)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = util.cos_sim(emb_original, emb_retranslated)
        
        # ëŒ€ê°ì„  ê°’ ì¶”ì¶œ (ê° í…ìŠ¤íŠ¸ ìŒì˜ ìœ ì‚¬ë„)
        return [similarities[i][i].item() for i in range(len(original_texts))]


class LanguageDetector:
    """
    ì–¸ì–´ ê°ì§€ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
    
    í…ìŠ¤íŠ¸ì— í¬í•¨ëœ íŠ¹ì • ì–¸ì–´ì˜ ê³ ìœ  ë¬¸ìë¥¼ ê²€ì‚¬í•˜ì—¬ ì–¸ì–´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
    """
    
    @staticmethod
    def is_french(text: str) -> bool:
        """í”„ë‘ìŠ¤ì–´ ê°ì§€ (Ã , Ã¢, Ã§, Ã¨, Ã©, Ãª, Ã®, Ã´, Ã¹, Ã», Å“ ë“±)"""
        if not isinstance(text, str):
            return False
        return bool(re.search(
            r'[\u00e0\u00e2\u00e7\u00e8\u00e9\u00ea\u00ee\u00f4\u0153\u00f9\u00fb'  # ì†Œë¬¸ì
            r'\u00c0\u00c2\u00c7\u00c8\u00c9\u00ca\u00ce\u00d4\u0152\u00d9\u00db]', # ëŒ€ë¬¸ì
            text
        ))
    
    @staticmethod
    def is_korean(text: str) -> bool:
        """í•œêµ­ì–´ ê°ì§€ (í•œê¸€)"""
        if not isinstance(text, str):
            return False
        return bool(re.search(r'[\uac00-\ud7af]', text))
    
    @staticmethod
    def is_japanese(text: str) -> bool:
        """ì¼ë³¸ì–´ ê°ì§€ (íˆë¼ê°€ë‚˜, ê°€íƒ€ì¹´ë‚˜, í•œì)"""
        if not isinstance(text, str):
            return False
        return bool(re.search(r'[\u3040-\u30ff\u4e00-\u9fff]', text))
    
    @staticmethod
    def is_chinese(text: str) -> bool:
        """ì¤‘êµ­ì–´ ê°ì§€ (í•œì)"""
        if not isinstance(text, str):
            return False
        return bool(re.search(r'[\u4e00-\u9fff]', text))
    
    @staticmethod
    def is_english(text: str) -> bool:
        """ì˜ì–´ ê°ì§€"""
        if not isinstance(text, str):
            return False
        return bool(re.search(r'[a-zA-Z]', text))
    
    @staticmethod
    def is_german(text: str) -> bool:
        """ë…ì¼ì–´ ê°ì§€ (Ã¤, Ã¶, Ã¼, ÃŸ ë“±)"""
        if not isinstance(text, str):
            return False
        return bool(re.search(r'[\u00e4\u00f6\u00fc\u00c4\u00d6\u00dc\u00df]', text))


def back_translate_data(
    input_csv_path: str,
    output_csv_path: str,
    api_key: str,
    label_value: int = 1,
    similarity_threshold: float = 0.7,
    max_similarity: float = 0.96,
    max_workers: int = 5
) -> pd.DataFrame:
    """
    ì—­ë²ˆì—­ì„ í†µí•œ ë°ì´í„° ì¦ê°•
    
    ì§€ì •ëœ ë¼ë²¨ì˜ í…ìŠ¤íŠ¸ë¥¼ ì—­ë²ˆì—­í•˜ì—¬ ì›ë³¸ ë°ì´í„°ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    ìœ ì‚¬ë„ê°€ ë‚®ê±°ë‚˜ ì¼ë³¸ì–´ê°€ í¬í•¨ëœ ê²½ìš° ìë™ìœ¼ë¡œ ì¬ë²ˆì—­í•©ë‹ˆë‹¤.
    
    Args:
        input_csv_path: ì…ë ¥ CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        output_csv_path: ì¶œë ¥ CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        api_key: OpenAI API í‚¤
        label_value: ì—­ë²ˆì—­í•  ë¼ë²¨ ê°’ (ê¸°ë³¸ê°’: 1)
        similarity_threshold: ìµœì†Œ ìœ ì‚¬ë„ (ê¸°ë³¸ê°’: 0.7)
        max_similarity: ìµœëŒ€ ìœ ì‚¬ë„ (ê¸°ë³¸ê°’: 0.96)
        max_workers: ë™ì‹œ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: 5)
        
    Returns:
        ì¦ê°•ëœ ë°ì´í„°í”„ë ˆì„
    """
    base_path = Path(__file__).parent.parent
    input_path = base_path / input_csv_path
    output_path = base_path / output_csv_path
    
    # ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {input_path}")
    data = pd.read_csv(input_path)
    
    # ì§€ì •ëœ ë¼ë²¨ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
    filtered = data[data["label"] == label_value].copy()
    print(f"ğŸ“Š ì—­ë²ˆì—­ ëŒ€ìƒ: {len(filtered)}ê°œ (ë¼ë²¨={label_value})")
    
    # ì—­ë²ˆì—­ê¸° ì´ˆê¸°í™”
    translator = BackTranslator(api_key=api_key, max_workers=max_workers)
    detector = LanguageDetector()
    
    # 1ì°¨ ì—­ë²ˆì—­
    print("ğŸ”„ 1ì°¨ ì—­ë²ˆì—­ ìˆ˜í–‰ ì¤‘...")
    texts = filtered["text"].tolist()
    japanese_results, korean_results = translator.translate_batch(texts)
    
    filtered['dialogue_translated'] = japanese_results
    filtered['dialogue_retranslated'] = korean_results
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    print("ğŸ“ ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
    similarities = translator.calculate_similarity(
        filtered["text"].tolist(),
        filtered["dialogue_retranslated"].tolist()
    )
    filtered["cosine_similarity"] = similarities
    
    # ì¬ë²ˆì—­ í•„ìš” ì—¬ë¶€ í™•ì¸
    has_japanese = filtered['dialogue_retranslated'].apply(detector.is_japanese)
    low_similarity = filtered['cosine_similarity'] < similarity_threshold
    needs_retry = has_japanese | low_similarity
    
    correct_df = filtered[~needs_retry]
    retry_df = filtered[needs_retry].copy()
    
    print(f"ğŸ“Š ì¬ë²ˆì—­ í•„ìš”: {len(retry_df)}ê°œ (ì¼ë³¸ì–´ {has_japanese.sum()}ê±´, ìœ ì‚¬ë„ ë¯¸ë‹¬ {low_similarity.sum()}ê±´)")
    
    # ì¬ë²ˆì—­ ìˆ˜í–‰
    if not retry_df.empty:
        print("ğŸ”„ ì¬ë²ˆì—­ ìˆ˜í–‰ ì¤‘...")
        retry_texts = retry_df["text"].tolist()
        retry_japanese, retry_korean = translator.translate_batch(retry_texts)
        
        retry_df['dialogue_translated'] = retry_japanese
        retry_df['dialogue_retranslated'] = retry_korean
        
        retry_similarities = translator.calculate_similarity(
            retry_df["text"].tolist(),
            retry_df["dialogue_retranslated"].tolist()
        )
        retry_df["cosine_similarity"] = retry_similarities
        
        final_df = pd.concat([correct_df, retry_df], ignore_index=True)
        print("âœ… ì¬ë²ˆì—­ ì™„ë£Œ")
    else:
        final_df = filtered.copy()
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì¡°ê±´ì„ ë§Œì¡±í•©ë‹ˆë‹¤")
    
    # ìœ ì‚¬ë„ í•„í„°ë§ (ì„ê³„ê°’ ë²”ìœ„ ë‚´ë§Œ ì„ íƒ)
    final_df = final_df[
        (final_df.cosine_similarity > similarity_threshold) & 
        (final_df.cosine_similarity < max_similarity)
    ]
    
    # ê²°ê³¼ ì •ë¦¬
    final_df = final_df[['dialogue_retranslated', 'label']].copy()
    final_df.columns = ['text', 'label']
    
    # ì›ë³¸ê³¼ ë³‘í•©
    augmented_df = pd.concat([data, final_df], ignore_index=True)
    
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {augmented_df.shape[0]}ê°œ (ì›ë³¸ {len(data)}ê°œ + ì¦ê°• {len(final_df)}ê°œ)")
    augmented_df.to_csv(output_path, index=False)
    
    return augmented_df


if __name__ == "__main__":
    import os
    
    # API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
    api_key = os.getenv("OPENAI_API_KEY", "")
    
    if not api_key:
        print("âš ï¸ ê²½ê³ : OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_keyë¥¼ ì§ì ‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        exit(1)
    
    # ì—­ë²ˆì—­ ìˆ˜í–‰
    augmented_data = back_translate_data(
        input_csv_path="dataset/stt_results_train.csv",
        output_csv_path="dataset/spam_stt_bt_fr.csv",
        api_key=api_key,
        label_value=1,
        similarity_threshold=0.7,
        max_similarity=0.96,
        max_workers=5
    )
    
    print("\nâœ… ì—­ë²ˆì—­ ì‘ì—… ì™„ë£Œ!")

