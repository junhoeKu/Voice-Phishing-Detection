"""
ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ì¶”ë¡  ëª¨ë“ˆ

í•™ìŠµëœ ë¶„ë¥˜ê¸° ëª¨ë¸ê³¼ ìƒì„±ê¸° ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë³´ì´ìŠ¤í”¼ì‹±ì„ íƒì§€í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ì›¹ UIë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    python inference/inference.py --adapter_path model/model_qwen_cls_bt_all_512_25
"""

import os
import torch
import torch.nn.functional as F
import argparse
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from peft import PeftModel
from huggingface_hub import HfFolder
import gradio as gr


def setup_environment(cache_dir: str = "cache", hf_token: str = ""):
    """
    í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    
    Args:
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        hf_token: Hugging Face í† í°
    """
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    base_path = Path(__file__).parent.parent
    cache_path = base_path / cache_dir
    cache_path.mkdir(parents=True, exist_ok=True)
    
    os.environ["TRANSFORMERS_CACHE"] = str(cache_path)
    
    if hf_token:
        os.environ["HUGGINGFACE_TOKEN"] = hf_token
        HfFolder.save_token(hf_token)


def load_models(
    base_model_path: str = "Qwen/Qwen2.5-0.5B-Instruct",
    adapter_path: str = "model/model_qwen_cls_bt_all_512_25",
    cache_dir: str = "cache",
    device: str = None,
    merge_adapter: bool = True
):
    """
    ë¶„ë¥˜ê¸°ì™€ ìƒì„±ê¸° ëª¨ë¸ ë¡œë“œ
    
    Args:
        base_model_path: ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ
        adapter_path: ì–´ëŒ‘í„° ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        merge_adapter: ì–´ëŒ‘í„° ë³‘í•© ì—¬ë¶€
        
    Returns:
        (classifier_model, generator_model, tokenizer) íŠœí”Œ
    """
    base_path = Path(__file__).parent.parent
    adapter_full_path = base_path / adapter_path
    cache_full_path = base_path / cache_dir
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    
    print(f"ğŸ“± ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ’¾ ìºì‹œ ë””ë ‰í† ë¦¬: {cache_full_path}")
    
    # 1. ë¶„ë¥˜ê¸° ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¥ ë¶„ë¥˜ê¸° ëª¨ë¸ ë¡œë”© ì¤‘...")
    classifier_model = AutoModelForSequenceClassification.from_pretrained(
        base_model_path,
        num_labels=2,
        torch_dtype=torch_dtype,
        cache_dir=str(cache_full_path),
        low_cpu_mem_usage=True
    )
    
    # PEFT ì–´ëŒ‘í„° ë¡œë“œ
    if not adapter_full_path.exists():
        raise FileNotFoundError(f"ì–´ëŒ‘í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adapter_full_path}")
    
    classifier_model = PeftModel.from_pretrained(classifier_model, str(adapter_full_path))
    
    # ì–´ëŒ‘í„° ë³‘í•© (ì„ íƒì )
    if merge_adapter:
        print("ğŸ”— ì–´ëŒ‘í„° ë³‘í•© ì¤‘...")
        classifier_model = classifier_model.merge_and_unload()
    
    classifier_model.eval()
    classifier_model.to(device)
    print("âœ… ë¶„ë¥˜ê¸° ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 2. ìƒì„±ê¸° ëª¨ë¸ ë¡œë“œ
    print("ğŸ“¥ ìƒì„±ê¸° ëª¨ë¸ ë¡œë”© ì¤‘...")
    generator_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch_dtype,
        cache_dir=str(cache_full_path),
        low_cpu_mem_usage=True
    )
    generator_model.eval()
    generator_model.to(device)
    print("âœ… ìƒì„±ê¸° ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ“¥ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        cache_dir=str(cache_full_path),
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'  # generateë¥¼ ìœ„í•´ padding sideë¥¼ leftë¡œ ì„¤ì •
    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    return classifier_model, generator_model, tokenizer


def preprocess_input(text: str) -> str:
    """
    ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        
    Returns:
        ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
    """
    return " ".join(text.strip().split())


def generate_response(
    prompt: str,
    classifier_model,
    generator_model,
    tokenizer,
    device: str,
    max_length: int = 1024,
    max_new_tokens: int = 200,
    min_new_tokens: int = 30
) -> str:
    """
    ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ë° ë¶„ì„ ìˆ˜í–‰
    
    Args:
        prompt: ì…ë ¥ í…ìŠ¤íŠ¸
        classifier_model: ë¶„ë¥˜ê¸° ëª¨ë¸
        generator_model: ìƒì„±ê¸° ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        device: ë””ë°”ì´ìŠ¤
        max_length: ìµœëŒ€ ì…ë ¥ ê¸¸ì´
        max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        min_new_tokens: ìµœì†Œ ìƒì„± í† í° ìˆ˜
        
    Returns:
        ë¶„ì„ ê²°ê³¼ (Markdown í˜•ì‹)
    """
    # 1ë‹¨ê³„: ë¶„ë¥˜ ìˆ˜í–‰
    processed_prompt = preprocess_input(prompt)
    inputs = tokenizer(
        processed_prompt,
        return_tensors="pt",
        truncation=True,
        max_length=max_length,
        padding=True
    ).to(device)
    
    with torch.no_grad():
        outputs = classifier_model(**inputs)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=1)
        prediction_idx = torch.argmax(probabilities, dim=1).item()
    
    if prediction_idx == 1:
        classification_result = "ë³´ì´ìŠ¤í”¼ì‹±"
        confidence = probabilities[0][1].item()
    else:
        classification_result = "ì •ìƒ ëŒ€í™”"
        confidence = probabilities[0][0].item()
    
    # 2ë‹¨ê³„: ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
    role = "ë‹¹ì‹ ì€ ë³´ì´ìŠ¤í”¼ì‹± í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ëŠ” AI ìˆ˜ì‚¬ê´€ì…ë‹ˆë‹¤."
    context = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” 1ì°¨ ë¶„ì„ ê²°ê³¼ '{classification_result}'ë¡œ íŒë³„ë˜ì—ˆìŠµë‹ˆë‹¤."
    task = f"ì´ í…ìŠ¤íŠ¸ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬, '{classification_result}' íŒë³„ì´ íƒ€ë‹¹í•œì§€ ê²€ì¦í•˜ê³ , ê·¸ í•µì‹¬ ê·¼ê±°ë¥¼ 'ë³´ì´ìŠ¤í”¼ì‹± íŒ¨í„´'(ê¸°ê´€ ì‚¬ì¹­, ê¸´ê¸‰ì„±, ê¸ˆì „/ì •ë³´ ìš”êµ¬ ë“±)ì— ê¸°ë°˜í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”."
    output_format = "ë°˜ë“œì‹œ [íŒë‹¨ ê·¼ê±°] 3ê°œì™€ [ê²°ë¡ ]ìœ¼ë¡œ êµ¬ì„±ëœ ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ì •ìƒ ëŒ€í™”ë¼ë©´ ì •ìƒ ëŒ€í™”ì¸ ì´ìœ ë¥¼ ë¶„ì„í•´ì„œ ì‘ì„±í•˜ì„¸ìš”."
    
    reasoning_prompt = f"""{role}
{context}

[ì„ë¬´]
{task}
{output_format}

[ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸]
{processed_prompt}

[ë¶„ì„ ë³´ê³ ì„œ]
"""
    
    # ìƒì„±ê¸° í† í¬ë‚˜ì´ì§•
    gen_inputs = tokenizer(reasoning_prompt, return_tensors="pt").to(device)
    
    # í…ìŠ¤íŠ¸ ìƒì„±
    gen_outputs = generator_model.generate(
        **gen_inputs,
        max_new_tokens=max_new_tokens,
        min_new_tokens=min_new_tokens,
        num_beams=3,
        do_sample=True,
        top_p=0.8,
        temperature=0.7,
        no_repeat_ngram_size=3,
        repetition_penalty=1.5,
        early_stopping=True,
        pad_token_id=tokenizer.eos_token_id,
    )
    
    # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
    generated_text = tokenizer.decode(
        gen_outputs[0][gen_inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    ).strip()
    
    # ê²°ê³¼ í¬ë§·íŒ…
    final_output = f"## 1. ë¶„ì„ ê²°ê³¼:\n"
    final_output += f"**{classification_result}** (ì‹ ë¢°ë„: {confidence:.2%})\n\n"
    final_output += f"## 2. ë¶„ì„ ë³´ê³ ì„œ (AI ìˆ˜ì‚¬ê´€):\n"
    final_output += f"{generated_text}"
    
    return final_output


def create_gradio_interface(
    classifier_model,
    generator_model,
    tokenizer,
    device: str
):
    """
    Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±
    
    Args:
        classifier_model: ë¶„ë¥˜ê¸° ëª¨ë¸
        generator_model: ìƒì„±ê¸° ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        device: ë””ë°”ì´ìŠ¤
        
    Returns:
        Gradio ì¸í„°í˜ì´ìŠ¤
    """
    def inference_fn(prompt: str) -> str:
        """Gradioìš© ì¶”ë¡  í•¨ìˆ˜"""
        if not prompt or not prompt.strip():
            return "âš ï¸ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        try:
            return generate_response(
                prompt,
                classifier_model,
                generator_model,
                tokenizer,
                device
            )
        except Exception as e:
            return f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    interface = gr.Interface(
        fn=inference_fn,
        inputs=[gr.Textbox(
            lines=10,
            placeholder="ë¶„ì„í•  ëŒ€í™”ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
            label="ëŒ€í™” ë‚´ìš©"
        )],
        outputs=gr.Markdown(label="ë¶„ì„ ê²°ê³¼"),
        title="ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ AI (ë¶„ë¥˜ ë° ë¶„ì„)",
        description="ì…ë ¥í•œ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ì´ìŠ¤í”¼ì‹± ì—¬ë¶€ë¥¼ 'ë¶„ë¥˜'í•˜ê³ , 'ì´ìœ 'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
        examples=[
            "ì €ëŠ” ì„œìš¸ì¤‘ì•™ì§€ê²€ ì²¨ë‹¨ë²”ì£„ìˆ˜ì‚¬ 1íŒ€ ê¹€ìƒìˆ˜ ìˆ˜ì‚¬ê´€ì…ë‹ˆë‹¤. ë³¸ì¸ ë§ìœ¼ì‹­ë‹ˆê¹Œ?",
            "ì´ì œ ì•Œë°”ë¥¼ êµ¬í•˜ë ¤ë‹¤ê°€ ê·¸ ë©”ê°€ìŠ¤í„°ë”” ëŸ¬ì…€í•™ì›ì´ë¼ê³  ìˆëŠ”ë°, ì•ˆì–‘ì—ìš”."
        ]
    )
    
    return interface


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ì¶”ë¡ ")
    parser.add_argument(
        "--base_model",
        type=str,
        default="Qwen/Qwen2.5-0.5B-Instruct",
        help="ê¸°ë³¸ ëª¨ë¸ ê²½ë¡œ"
    )
    parser.add_argument(
        "--adapter_path",
        type=str,
        default="model/model_qwen_cls_bt_all_512_25",
        help="ì–´ëŒ‘í„° ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)"
    )
    parser.add_argument(
        "--cache_dir",
        type=str,
        default="cache",
        help="ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="ë””ë°”ì´ìŠ¤ (cuda/cpu, Noneì´ë©´ ìë™ ì„ íƒ)"
    )
    parser.add_argument(
        "--hf_token",
        type=str,
        default="",
        help="Hugging Face í† í° (í™˜ê²½ë³€ìˆ˜ HUGGINGFACE_TOKENì—ì„œë„ ì½ì„ ìˆ˜ ìˆìŒ)"
    )
    parser.add_argument(
        "--no_merge",
        action="store_true",
        help="ì–´ëŒ‘í„° ë³‘í•©í•˜ì§€ ì•Šê¸°"
    )
    parser.add_argument(
        "--share",
        action="store_true",
        help="Gradio ê³µìœ  ë§í¬ ìƒì„±"
    )
    parser.add_argument(
        "--server_name",
        type=str,
        default="127.0.0.1",
        help="ì„œë²„ ì£¼ì†Œ"
    )
    parser.add_argument(
        "--server_port",
        type=int,
        default=7860,
        help="ì„œë²„ í¬íŠ¸"
    )
    
    args = parser.parse_args()
    
    # Hugging Face í† í° ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
    hf_token = os.getenv("HUGGINGFACE_TOKEN", args.hf_token)
    
    # í™˜ê²½ ì„¤ì •
    setup_environment(args.cache_dir, hf_token)
    
    # ëª¨ë¸ ë¡œë“œ
    classifier_model, generator_model, tokenizer = load_models(
        base_model_path=args.base_model,
        adapter_path=args.adapter_path,
        cache_dir=args.cache_dir,
        device=args.device,
        merge_adapter=not args.no_merge
    )
    
    device = args.device if args.device else ("cuda" if torch.cuda.is_available() else "cpu")
    
    # Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
    print("ğŸš€ Gradio ì¸í„°í˜ì´ìŠ¤ ì‹œì‘ ì¤‘...")
    interface = create_gradio_interface(
        classifier_model,
        generator_model,
        tokenizer,
        device
    )
    
    interface.launch(
        share=args.share,
        server_name=args.server_name,
        server_port=args.server_port
    )


if __name__ == "__main__":
    main()

