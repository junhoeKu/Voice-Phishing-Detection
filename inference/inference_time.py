"""
LLM ì¶”ë¡  ì‹œê°„ ì¸¡ì • ëª¨ë“ˆ

ì—¬ëŸ¬ LLM ëª¨ë¸ì˜ ì¶”ë¡  ì‹œê°„ì„ ì¸¡ì •í•˜ê³  ë²¤ì¹˜ë§ˆí¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ê° ëª¨ë¸ ì¸¡ì • í›„ ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    python inference/inference_time.py --prompt "ì•ˆë…•í•˜ì„¸ìš”" --max_new_tokens 100
    python inference/inference_time.py --models Qwen/Qwen2.5-0.5B-Instruct maywell/Synatra-42dot-1.3B
"""

import os
import gc
import time
import torch
import argparse
import pandas as pd
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer


def setup_environment(cache_dir: str = "cache"):
    """
    í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    
    Args:
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
    """
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    
    base_path = Path(__file__).parent.parent
    cache_path = base_path / cache_dir
    cache_path.mkdir(parents=True, exist_ok=True)
    
    os.environ["TRANSFORMERS_CACHE"] = str(cache_path)
    os.environ["HUGGINGFACE_HUB_CACHE"] = str(cache_path)


def measure_inference_time(
    model_id: str,
    prompt: str,
    max_new_tokens: int,
    cache_dir: str = "cache",
    device: str = None,
    warmup_tokens: int = 10
) -> float:
    """
    ì§€ì •ëœ ëª¨ë¸ IDë¡œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³ , ì›Œë°ì—… í›„ ìˆœìˆ˜ ì¶”ë¡  ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    
    Args:
        model_id: ëª¨ë¸ ID (Hugging Face ëª¨ë¸ëª…)
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        warmup_tokens: ì›Œë°ì—… ì‹œ ìƒì„±í•  í† í° ìˆ˜
        
    Returns:
        ì¶”ë¡  ì‹œê°„ (ì´ˆ), ì‹¤íŒ¨ ì‹œ -1.0
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    base_path = Path(__file__).parent.parent
    cache_path = base_path / cache_dir
    
    print(f"\n[ëª¨ë¸ ë¡œë”©]: {model_id}")
    model = None
    tokenizer = None
    
    try:
        # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=str(cache_path))
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch_dtype,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            cache_dir=str(cache_path)
        ).to(device)
        model.eval()
        
        # 2. íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 3. ì…ë ¥ ì¤€ë¹„
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # 4. ì›Œë°ì—… (Warm-up)
        print("... ì›Œë°ì—… 1íšŒ ì‹¤í–‰ ì¤‘ ...")
        _ = model.generate(
            **inputs,
            max_new_tokens=warmup_tokens,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=False
        )
        
        # 5. ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        if device == "cuda":
            torch.cuda.synchronize()
        
        start_time = time.perf_counter()
        
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=False
        )
        
        if device == "cuda":
            torch.cuda.synchronize()
        
        end_time = time.perf_counter()
        duration = end_time - start_time
        
        # 6. ê²°ê³¼ ë””ì½”ë”©
        output_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        
        print(f"  [ìƒì„± ê²°ê³¼]: ...{output_text.strip()[:50]}")
        print(f"  [ì¶”ë¡  ì‹œê°„]: {duration:.4f} ì´ˆ")
        
        return duration
        
    except Exception as e:
        print(f"  [ì˜¤ë¥˜ ë°œìƒ]: {model_id} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - {e}")
        # VRAM ë¶€ì¡±(OOM) ì˜¤ë¥˜ê°€ ê°€ì¥ í”í•œ ì›ì¸ì…ë‹ˆë‹¤.
        if "out of memory" in str(e).lower() or "oom" in str(e).lower():
            print("  [ì›ì¸]: ğŸ”´ GPU VRAM(ë©”ëª¨ë¦¬) ë¶€ì¡± (OOM) ğŸ”´")
        return -1.0
        
    finally:
        # ë©”ëª¨ë¦¬ í™•ë³´ë¥¼ ìœ„í•´ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ í•´ì œ
        del model, tokenizer
        gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        if device == "cuda":
            torch.cuda.empty_cache()
        print(f"[ë©”ëª¨ë¦¬ í•´ì œ]: {model_id} ì™„ë£Œ")


def run_benchmark(
    model_ids: list,
    prompt: str,
    max_new_tokens: int,
    cache_dir: str = "cache",
    device: str = None,
    warmup_tokens: int = 10
) -> dict:
    """
    ì—¬ëŸ¬ ëª¨ë¸ì— ëŒ€í•´ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    Args:
        model_ids: í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ID ë¦¬ìŠ¤íŠ¸
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        device: ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ)
        warmup_tokens: ì›Œë°ì—… ì‹œ ìƒì„±í•  í† í° ìˆ˜
        
    Returns:
        {ëª¨ë¸_ID: ì¶”ë¡ ì‹œê°„} ë”•ì…”ë„ˆë¦¬
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32
    print(f"ğŸš€ ë””ë°”ì´ìŠ¤: {device}")
    print(f"âš¡ ë°ì´í„° íƒ€ì…: {torch_dtype}")
    print("-------------------------------------------------")
    
    results = {}
    print("======== LLM ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ========")
    
    for model_id in model_ids:
        duration = measure_inference_time(
            model_id=model_id,
            prompt=prompt,
            max_new_tokens=max_new_tokens,
            cache_dir=cache_dir,
            device=device,
            warmup_tokens=warmup_tokens
        )
        results[model_id] = duration
        print("-" * 40)
    
    return results


def print_results(results: dict, prompt: str, max_new_tokens: int, device: str):
    """
    ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        results: {ëª¨ë¸_ID: ì¶”ë¡ ì‹œê°„} ë”•ì…”ë„ˆë¦¬
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        device: ë””ë°”ì´ìŠ¤
    """
    print("\n" + "="*40)
    print("          ğŸ“Š ìµœì¢… ë¹„êµ ê²°ê³¼ ğŸ“Š")
    print("="*40)
    print(f"(Prompt: '{prompt}')")
    print(f"(Max New Tokens: {max_new_tokens}, Device: {device.upper()})\n")
    
    for model_id, duration in results.items():
        if duration == -1.0:
            print(f"ëª¨ë¸: {model_id}\n  ê²°ê³¼: âŒ ë¡œë“œ ë˜ëŠ” ì¶”ë¡  ì‹¤íŒ¨ (OOM ê°€ëŠ¥ì„±)\n")
        else:
            print(f"ëª¨ë¸: {model_id}\n  ì‹œê°„: {duration:.4f} ì´ˆ\n")
    
    print("="*40)
    print("âš ï¸ 7B ì´ìƒ ëª¨ë¸ì€ T4 í™˜ê²½ì—ì„œ VRAM ë¶€ì¡±(OOM)ìœ¼ë¡œ")
    print("   ì‹¤íŒ¨í–ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. (T4 VRAM: ì•½ 15GB)")
    print("   ë” í° GPU(A100/H100)ê°€ ìˆëŠ” í™˜ê²½ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


def save_results(results: dict, prompt: str, max_new_tokens: int, device: str, output_path: str):
    """
    ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        results: {ëª¨ë¸_ID: ì¶”ë¡ ì‹œê°„} ë”•ì…”ë„ˆë¦¬
        prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
        max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        device: ë””ë°”ì´ìŠ¤
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
    """
    base_path = Path(__file__).parent.parent
    output_full_path = base_path / output_path
    output_full_path.parent.mkdir(parents=True, exist_ok=True)
    
    df = pd.DataFrame([
        {
            "model_id": model_id,
            "inference_time": duration if duration != -1.0 else None,
            "status": "success" if duration != -1.0 else "failed",
            "prompt": prompt,
            "max_new_tokens": max_new_tokens,
            "device": device
        }
        for model_id, duration in results.items()
    ])
    
    df.to_csv(output_full_path, index=False, encoding="utf-8-sig")
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_full_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="LLM ì¶”ë¡  ì‹œê°„ ì¸¡ì • ë²¤ì¹˜ë§ˆí¬")
    parser.add_argument("--prompt", type=str, default="ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ AI ëª¨ë¸", help="ì…ë ¥ í”„ë¡¬í”„íŠ¸")
    parser.add_argument("--max_new_tokens", type=int, default=100, help="ìµœëŒ€ ìƒì„± í† í° ìˆ˜")
    parser.add_argument("--models", type=str, nargs="+", default=[
        "Qwen/Qwen2.5-0.5B-Instruct",
        "maywell/Synatra-42dot-1.3B",
        "Qwen/Qwen2.5-7B-Instruct",
        "Qwen/Qwen2.5-14B-Instruct"
    ], help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ID ë¦¬ìŠ¤íŠ¸")
    parser.add_argument("--cache_dir", type=str, default="cache", help="ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--device", type=str, default=None, help="ë””ë°”ì´ìŠ¤ (cuda/cpu, Noneì´ë©´ ìë™ ì„ íƒ)")
    parser.add_argument("--warmup_tokens", type=int, default=10, help="ì›Œë°ì—… ì‹œ ìƒì„±í•  í† í° ìˆ˜")
    parser.add_argument("--output", type=str, default="results/inference_time_results.csv", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    args = parser.parse_args()
    
    setup_environment(args.cache_dir)
    device = args.device if args.device else ("cuda" if torch.cuda.is_available() else "cpu")
    
    results = run_benchmark(
        model_ids=args.models,
        prompt=args.prompt,
        max_new_tokens=args.max_new_tokens,
        cache_dir=args.cache_dir,
        device=device,
        warmup_tokens=args.warmup_tokens
    )
    
    print_results(results, args.prompt, args.max_new_tokens, device)
    
    if args.output:
        save_results(results, args.prompt, args.max_new_tokens, device, args.output)


if __name__ == "__main__":
    main()

