{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNScS0p1bYGSa6hOEFY04o1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"95g0_QqJMTqx"},"outputs":[],"source":["import os\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from peft import PeftModel\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from data_loader import load_voicephishing_data\n","from datasets import load_dataset\n","\n","\n","# pip install --upgrade -q datasets peft transformers==4.45.2\n","seed = 42\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)  # ë©€í‹° GPU ì‚¬ìš© ì‹œ\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","\n","# ===== ê³ ì • ê²½ë¡œ ì„¤ì • =====\n","BASE_DIR = \"/home/work/Voicephishing\"\n","CACHE_DIR = os.path.join(BASE_DIR, \"cache\")\n","BASE_MODEL_PATH = \"maywell/Synatra-42dot-1.3B\"\n","ADAPTER_PATH = os.path.join(BASE_DIR, \"model/model_synatra_cls_251021\")\n","TOKENIZER_PATH = os.path.join(BASE_DIR, \"tokenizer/tokenizer_synatra_cls_251021\")\n","TEST_DATA_PATH = os.path.join(BASE_DIR, \"dataset/korccvi_256_0_test.csv\")\n","SAVE_PATH = os.path.join(BASE_DIR, \"dataset/eval_folder/eval_synatra_cls_251021\")\n","SCORES_PATH = os.path.join(BASE_DIR, \"dataset/eval_folder/eval_metrics.csv\")\n","\n","# ===== í™˜ê²½ ë³€ìˆ˜ =====\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n","os.environ[\"TRITON_CACHE_DIR\"] = os.path.join(CACHE_DIR, \"triton\")\n","os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/home/work/Voicephishing/hf_cache\"\n","\n","# ===== ì „ì²˜ë¦¬ í•¨ìˆ˜ =====\n","def preprocess_input(text):\n","    text = \" \".join(text.strip().split())\n","    return text\n","\n","# ===== í‰ê°€ í•¨ìˆ˜ =====\n","def evaluate_and_save():\n","    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, cache_dir=CACHE_DIR)\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    # ì €ì¥ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ ë¡œë“œ\n","    test_dataset = load_dataset(\"csv\", data_files=TEST_DATA_PATH)[\"train\"]\n","\n","    base_model = AutoModelForSequenceClassification.from_pretrained(\n","        BASE_MODEL_PATH,\n","        num_labels=2,\n","        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.bfloat16,\n","        cache_dir=CACHE_DIR,\n","        low_cpu_mem_usage=True,\n","        device_map=\"cuda:0\"\n","    )\n","\n","    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n","    model = model.merge_and_unload()\n","    model.eval()\n","    for p in model.parameters():\n","        p.requires_grad = False\n","\n","    records = []\n","    y_true = []\n","    y_pred = []\n","\n","    label_map = {0: \"ì •ìƒ\", 1: \"ë³´ì´ìŠ¤í”¼ì‹±\"}\n","    inv_label_map = {\"ì •ìƒ\": 0, \"ë³´ì´ìŠ¤í”¼ì‹±\": 1}\n","\n","    for sample in tqdm(test_dataset, desc=\"ğŸ§ª Evaluating\"):\n","        input_text = sample[\"text\"]\n","        target_label = label_map[sample[\"Label\"]]\n","\n","        preprocessed_text = preprocess_input(input_text)\n","        inputs = tokenizer(preprocessed_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1024, return_token_type_ids=False)\n","        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n","\n","        with torch.no_grad():\n","            logits = model(**inputs).logits\n","            predicted_label = torch.argmax(logits, dim=-1).item()\n","            # predicted_label = 1 if logits.item() > 0 else 0\n","\n","        pred_text = label_map[predicted_label]\n","        is_correct = (pred_text == target_label)\n","\n","        y_true.append(inv_label_map[target_label])\n","        y_pred.append(inv_label_map[pred_text])\n","\n","        records.append({\n","            \"Input\": input_text,\n","            \"Prediction\": pred_text,\n","            \"Label\": target_label,\n","            \"Correct\": int(is_correct)\n","        })\n","\n","    # ===== ì§€í‘œ ê³„ì‚° =====\n","    acc = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, zero_division=0)\n","    recall = recall_score(y_true, y_pred, zero_division=0)\n","    f1 = f1_score(y_true, y_pred, zero_division=0)\n","\n","    # ===== ê²°ê³¼ ì¶œë ¥ =====\n","    scores_df = pd.DataFrame([{\n","        \"Accuracy\": acc,\n","        \"Precision\": precision,\n","        \"Recall\": recall,\n","        \"F1 Score\": f1\n","    }])\n","\n","    print(\"\\nâœ… í‰ê°€ ì§€í‘œ:\")\n","    print(scores_df.to_string(index=False, float_format=\"%.4f\"))\n","\n","    # ===== ê²°ê³¼ ì €ì¥ =====\n","    os.makedirs(os.path.dirname(SAVE_PATH), exist_ok=True)\n","    pd.DataFrame(records).to_csv(SAVE_PATH, index=False, encoding=\"utf-8-sig\")\n","    scores_df.to_csv(SCORES_PATH, index=False, encoding=\"utf-8-sig\")\n","\n","    print(f\"\\nğŸ“ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {SAVE_PATH}\")\n","    print(f\"ğŸ“ í‰ê°€ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {SCORES_PATH}\")\n","\n","# ===== ì‹¤í–‰ =====\n","if __name__ == \"__main__\":\n","    evaluate_and_save()\n"]}]}