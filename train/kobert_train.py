"""
KoBERT ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

KoBERTë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
5íšŒ ì‹¤í–‰ì˜ í‰ê·  ì„±ëŠ¥ì„ ê³„ì‚°í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    python train/kobert_train.py --data dataset/total_dataset.csv
"""

import os
import random
import argparse
import pandas as pd
import numpy as np
import torch
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback
)
from datasets import Dataset
import wandb


def set_seed(seed_value: int = 42):
    """ë‚œìˆ˜ ì‹œë“œ ê³ ì •"""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def load_data(data_path: str, text_column: str = "text", label_column: str = "label"):
    """
    ë°ì´í„° ë¡œë“œ
    
    Args:
        data_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
        label_column: ë¼ë²¨ ì»¬ëŸ¼ëª…
        
    Returns:
        (X, y) íŠœí”Œ
    """
    base_path = Path(__file__).parent.parent
    full_path = base_path / data_path
    
    if not full_path.exists():
        raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")
    
    df = pd.read_csv(full_path)
    df = df.dropna(subset=[text_column, label_column])
    
    X = df[text_column].astype(str).tolist()
    y = df[label_column].astype(int).tolist()
    
    print(f"ğŸ“Š ì „ì²´ ë°ì´í„°: {len(df)}ê°œ")
    return X, y


def compute_metrics(eval_pred):
    """í‰ê°€ ì§€í‘œ ê³„ì‚°"""
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, zero_division=0)
    precision = precision_score(labels, preds, zero_division=0)
    recall = recall_score(labels, preds, zero_division=0)
    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }


def train_and_evaluate(
    X_all,
    y_all,
    num_runs: int = 5,
    learning_rate: float = 2e-5,
    train_batch_size: int = 16,
    eval_batch_size: int = 32,
    epochs: int = 5,
    max_length: int = 256,
    early_stopping: int = 3,
    seed: int = 42,
    cache_dir: str = "cache",
    output_dir: str = "results/kobert",
    log_dir: str = "logs/kobert"
):
    """
    ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
    
    Args:
        X_all: ì „ì²´ í…ìŠ¤íŠ¸ ë°ì´í„°
        y_all: ì „ì²´ ë¼ë²¨ ë°ì´í„°
        num_runs: ì‹¤í–‰ íšŸìˆ˜
        learning_rate: í•™ìŠµë¥ 
        train_batch_size: í•™ìŠµ ë°°ì¹˜ í¬ê¸°
        eval_batch_size: í‰ê°€ ë°°ì¹˜ í¬ê¸°
        epochs: ì—í¬í¬ ìˆ˜
        max_length: ìµœëŒ€ ê¸¸ì´
        early_stopping: ì¡°ê¸° ì¢…ë£Œ patience
        seed: ì‹œë“œ ê°’
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ (ìƒëŒ€ê²½ë¡œ)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ìƒëŒ€ê²½ë¡œ)
        log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬ (ìƒëŒ€ê²½ë¡œ)
        
    Returns:
        (ëª¨ë¸, í† í¬ë‚˜ì´ì €, í‰ê°€_ê²°ê³¼) íŠœí”Œ
    """
    base_path = Path(__file__).parent.parent
    cache_full_path = base_path / cache_dir
    output_full_path = base_path / output_dir
    log_full_path = base_path / log_dir
    
    cache_full_path.mkdir(parents=True, exist_ok=True)
    output_full_path.mkdir(parents=True, exist_ok=True)
    log_full_path.mkdir(parents=True, exist_ok=True)
    
    val_metrics = {"accuracy": [], "precision": [], "recall": [], "f1": []}
    test_metrics = {"accuracy": [], "precision": [], "recall": [], "f1": []}
    
    last_model = None
    last_tokenizer = None
    
    for run_idx in range(num_runs):
        run_seed = seed + run_idx
        set_seed(run_seed)
        
        # ë°ì´í„° ë¶„í• : 70% train, 30% temp
        X_train, X_temp, y_train, y_temp = train_test_split(
            X_all,
            y_all,
            test_size=0.30,
            random_state=run_seed,
            stratify=y_all
        )
        
        # tempì„ 15% val, 15% testë¡œ ë¶„í• 
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp,
            y_temp,
            test_size=0.50,
            random_state=run_seed,
            stratify=y_temp
        )
        
        print(f"[Run {run_idx+1}] Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        train_ds = [{"text": x, "label": int(y)} for x, y in zip(X_train, y_train)]
        val_ds = [{"text": x, "label": int(y)} for x, y in zip(X_val, y_val)]
        test_ds = [{"text": x, "label": int(y)} for x, y in zip(X_test, y_test)]
        
        # í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (ë§¤ runë§ˆë‹¤ ìƒˆë¡œ ë¡œë“œ)
        tokenizer = BertTokenizer.from_pretrained("monologg/kobert", cache_dir=str(cache_full_path))
        model = BertForSequenceClassification.from_pretrained("monologg/kobert", num_labels=2)
        model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
        
        # í† í°í™”
        def tokenize_fn(batch):
            return tokenizer(
                batch["text"],
                max_length=max_length,
                truncation=True,
                padding="max_length"
            )
        
        train_dataset = Dataset.from_list(train_ds).map(tokenize_fn, batched=True)
        val_dataset = Dataset.from_list(val_ds).map(tokenize_fn, batched=True)
        test_dataset = Dataset.from_list(test_ds).map(tokenize_fn, batched=True)
        
        train_dataset = train_dataset.rename_column("label", "labels")
        val_dataset = val_dataset.rename_column("label", "labels")
        test_dataset = test_dataset.rename_column("label", "labels")
        
        train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
        val_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
        test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
        
        # Data collator
        data_collator = DataCollatorWithPadding(tokenizer)
        
        # Training arguments
        training_args = TrainingArguments(
            output_dir=str(output_full_path),
            overwrite_output_dir=True,
            learning_rate=learning_rate,
            per_device_train_batch_size=train_batch_size,
            per_device_eval_batch_size=eval_batch_size,
            num_train_epochs=epochs,
            seed=run_seed,
            warmup_ratio=0.1,
            weight_decay=0.01,
            logging_dir=str(log_full_path),
            logging_steps=50,
            eval_strategy="epoch",
            save_strategy="epoch",
            save_total_limit=1,
            load_best_model_at_end=True,
            metric_for_best_model="accuracy",
            greater_is_better=True,
            report_to="wandb",
            disable_tqdm=True
        )
        
        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
            callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping)]
        )
        
        # í•™ìŠµ
        print(f"[Run {run_idx+1}] í•™ìŠµ ì¤‘...")
        trainer.train()
        
        # Validation í‰ê°€
        val_metrics_run = trainer.evaluate(val_dataset)
        print(f"[Run {run_idx+1}] Validation metrics: {val_metrics_run}")
        wandb.log({
            f"run_idx": run_idx + 1,
            **{f"val_{k}": v for k, v in val_metrics_run.items() if isinstance(v, float)}
        })
        
        for k in val_metrics.keys():
            val_metrics[k].append(val_metrics_run.get(f"eval_{k}", np.nan))
        
        # Test í‰ê°€
        test_metrics_run = trainer.evaluate(test_dataset)
        print(f"[Run {run_idx+1}] Test metrics: {test_metrics_run}")
        wandb.log({
            f"run_idx": run_idx + 1,
            **{f"test_{k}": v for k, v in test_metrics_run.items() if isinstance(v, float)}
        })
        
        for k in test_metrics.keys():
            test_metrics[k].append(test_metrics_run.get(f"eval_{k}", np.nan))
        
        # ë§ˆì§€ë§‰ runì˜ ëª¨ë¸/í† í¬ë‚˜ì´ì €ë§Œ ì €ì¥
        if run_idx == num_runs - 1:
            last_model = model
            last_tokenizer = tokenizer
    
    # í‰ê·  ê²°ê³¼ ê³„ì‚°
    def _mean(v):
        return float(np.mean([x for x in v if x is not None and not np.isnan(x)])) if len(v) > 0 else float("nan")
    
    val_avg = {k: _mean(val_metrics[k]) for k in val_metrics}
    test_avg = {k: _mean(test_metrics[k]) for k in test_metrics}
    
    print("\n=== Validation Averages over 5 runs ===")
    print(f"Val Accuracy: {val_avg['accuracy']:.4f}, Val Precision: {val_avg['precision']:.4f}, Val Recall: {val_avg['recall']:.4f}, Val F1: {val_avg['f1']:.4f}")
    
    print("\n=== Test Averages over 5 runs ===")
    print(f"Test Accuracy: {test_avg['accuracy']:.4f}, Test Precision: {test_avg['precision']:.4f}, Test Recall: {test_avg['recall']:.4f}, Test F1: {test_avg['f1']:.4f}")
    
    # wandb summaryì— í‰ê·  ê¸°ë¡
    wandb.summary["val_accuracy_mean"] = val_avg["accuracy"]
    wandb.summary["val_precision_mean"] = val_avg["precision"]
    wandb.summary["val_recall_mean"] = val_avg["recall"]
    wandb.summary["val_f1_mean"] = val_avg["f1"]
    
    wandb.summary["test_accuracy_mean"] = test_avg["accuracy"]
    wandb.summary["test_precision_mean"] = test_avg["precision"]
    wandb.summary["test_recall_mean"] = test_avg["recall"]
    wandb.summary["test_f1_mean"] = test_avg["f1"]
    
    return last_model, last_tokenizer, {"val": val_avg, "test": test_avg}


def save_model(model, tokenizer, model_path: str, tokenizer_path: str = None):
    """
    ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ì¥
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        model_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        tokenizer_path: í† í¬ë‚˜ì´ì € ì €ì¥ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ, Noneì´ë©´ ìë™ ìƒì„±)
    """
    base_path = Path(__file__).parent.parent
    model_full_path = base_path / model_path
    model_full_path.parent.mkdir(parents=True, exist_ok=True)
    
    model.save_pretrained(str(model_full_path))
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_full_path}")
    
    if tokenizer_path:
        tokenizer_full_path = base_path / tokenizer_path
        tokenizer_full_path.parent.mkdir(parents=True, exist_ok=True)
        tokenizer.save_pretrained(str(tokenizer_full_path))
        print(f"ğŸ’¾ í† í¬ë‚˜ì´ì € ì €ì¥: {tokenizer_full_path}")
    else:
        # í† í¬ë‚˜ì´ì €ë„ ê°™ì€ ê²½ë¡œì— ì €ì¥
        tokenizer.save_pretrained(str(model_full_path))
        print(f"ğŸ’¾ í† í¬ë‚˜ì´ì € ì €ì¥: {model_full_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="KoBERT ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
    parser.add_argument("--data", type=str, default="dataset/total_dataset.csv", help="ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--model_path", type=str, default="model/model_kobert", help="ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--tokenizer_path", type=str, default=None, help="í† í¬ë‚˜ì´ì € ì €ì¥ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ, Noneì´ë©´ model_path ì‚¬ìš©)")
    parser.add_argument("--num_runs", type=int, default=5, help="ì‹¤í–‰ íšŸìˆ˜")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="í•™ìŠµë¥ ")
    parser.add_argument("--train_batch_size", type=int, default=16, help="í•™ìŠµ ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--eval_batch_size", type=int, default=32, help="í‰ê°€ ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--epochs", type=int, default=5, help="ì—í¬í¬ ìˆ˜")
    parser.add_argument("--max_length", type=int, default=256, help="ìµœëŒ€ ê¸¸ì´")
    parser.add_argument("--early_stopping", type=int, default=3, help="ì¡°ê¸° ì¢…ë£Œ patience")
    parser.add_argument("--seed", type=int, default=42, help="ì‹œë“œ ê°’")
    parser.add_argument("--cache_dir", type=str, default="cache", help="ìºì‹œ ë””ë ‰í† ë¦¬ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--output_dir", type=str, default="results/kobert", help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--log_dir", type=str, default="logs/kobert", help="ë¡œê·¸ ë””ë ‰í† ë¦¬ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--wandb_project", type=str, default="Voicephishing", help="WandB í”„ë¡œì íŠ¸ëª…")
    parser.add_argument("--wandb_name", type=str, default=None, help="WandB ì‹¤í–‰ëª… (Noneì´ë©´ ìë™ ìƒì„±)")
    args = parser.parse_args()
    
    wandb.init(project=args.wandb_project, name=args.wandb_name or f"kobert_{args.num_runs}runs", config={
        "model": "KoBERT", "learning_rate": args.learning_rate, "train_batch_size": args.train_batch_size,
        "eval_batch_size": args.eval_batch_size, "epochs": args.epochs, "max_length": args.max_length,
        "early_stopping": args.early_stopping, "num_runs": args.num_runs, "split_ratio": "train 0.70, val 0.15, test 0.15"
    })
    
    try:
        X_all, y_all = load_data(args.data)
        model, tokenizer, metrics = train_and_evaluate(
            X_all=X_all, y_all=y_all, num_runs=args.num_runs, learning_rate=args.learning_rate,
            train_batch_size=args.train_batch_size, eval_batch_size=args.eval_batch_size, epochs=args.epochs,
            max_length=args.max_length, early_stopping=args.early_stopping, seed=args.seed,
            cache_dir=args.cache_dir, output_dir=args.output_dir, log_dir=args.log_dir
        )
        if model is not None:
            save_model(model, tokenizer, args.model_path, args.tokenizer_path)
            try:
                wandb.save(str(Path(__file__).parent.parent / args.model_path))
            except Exception:
                pass
        print("\nâœ… í•™ìŠµ ë° í‰ê°€ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        wandb.finish()


if __name__ == "__main__":
    main()

