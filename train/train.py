"""
ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Synatra-42dot-1.3B ëª¨ë¸ì„ LoRAë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.
ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
"""

import os
import torch
import random
import numpy as np
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding,
    BitsAndBytesConfig, EarlyStoppingCallback
)
from peft import get_peft_model, LoraConfig, TaskType
from huggingface_hub import HfFolder
import mlflow
from datasets import load_dataset
from pathlib import Path


def set_seed(seed_value=42):
    """ë‚œìˆ˜ ì‹œë“œ ê³ ì •"""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True


def setup_environment(
    cache_dir: str = "cache",
    mlruns_dir: str = "mlruns",
    hf_token: str = ""
):
    """
    í™˜ê²½ ë³€ìˆ˜ ë° ê²½ë¡œ ì„¤ì •
    
    Args:
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        mlruns_dir: MLflow runs ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        hf_token: Hugging Face í† í°
    """
    base_path = Path(__file__).parent.parent
    
    # ê²½ë¡œ ì„¤ì •
    cache_path = base_path / cache_dir
    mlruns_path = base_path / mlruns_dir
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    cache_path.mkdir(parents=True, exist_ok=True)
    mlruns_path.mkdir(parents=True, exist_ok=True)
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["TRANSFORMERS_CACHE"] = str(cache_path)
    os.environ["HUGGINGFACE_HUB_CACHE"] = str(cache_path)
    os.environ["TRITON_CACHE_DIR"] = str(cache_path / "triton")
    
    # GPU ì„¤ì •
    gpu_count = torch.cuda.device_count()
    if gpu_count > 0:
        os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(str(i) for i in range(gpu_count))
        print(f'Device: {torch.cuda.current_device()}')
        print(f'Using {torch.cuda.device_count()} GPUs')
    else:
        print('Using CPU')
    
    # Hugging Face ì¸ì¦
    if hf_token:
        os.environ["HUGGINGFACE_TOKEN"] = hf_token
        HfFolder.save_token(hf_token)
    
    # MLflow ì„¤ì •
    mlflow.set_tracking_uri(f"file:{mlruns_path}")
    mlflow.set_experiment("llm_experiment")
    
    return str(cache_path)


def load_model_and_tokenizer(
    model_name: str = "maywell/Synatra-42dot-1.3B",
    cache_dir: str = "cache",
    num_labels: int = 2,
    lora_r: int = 16,
    lora_alpha: int = 32,
    lora_dropout: float = 0.2,
    target_modules: list = None
):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ë° LoRA ì„¤ì •
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        num_labels: ë¶„ë¥˜ ë ˆì´ë¸” ìˆ˜
        lora_r: LoRA rank
        lora_alpha: LoRA alpha
        lora_dropout: LoRA dropout
        target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        (model, tokenizer) íŠœí”Œ
    """
    if target_modules is None:
        target_modules = ["q_proj", "v_proj"]
    
    base_path = Path(__file__).parent.parent
    cache_path = base_path / cache_dir
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir=str(cache_path),
        use_fast=False,
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # BitsAndBytes ì„¤ì • (4-bit ì–‘ìí™”)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        num_labels=num_labels,
        low_cpu_mem_usage=True,
        trust_remote_code=True,
        cache_dir=str(cache_path),
    )
    
    # LoRA ì„¤ì •
    lora_config = LoraConfig(
        task_type=TaskType.SEQ_CLS,
        inference_mode=False,
        r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules
    )
    
    # LoRA ì ìš©
    model = get_peft_model(model, lora_config)
    model.config.pad_token_id = tokenizer.pad_token_id
    model.gradient_checkpointing_enable()
    model.enable_input_require_grads()
    model.config.use_cache = False
    model.train()
    
    return model, tokenizer


def tokenize_voicephishing_data(dataset, tokenizer, max_length=1024):
    """
    ë°ì´í„°ì…‹ í† í°í™”
    
    Args:
        dataset: ë°ì´í„°ì…‹
        tokenizer: í† í¬ë‚˜ì´ì €
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        
    Returns:
        í† í°í™”ëœ ë°ì´í„°ì…‹
    """
    def tokenize_fn(batch):
        tokenized = tokenizer(
            batch["text"],
            max_length=max_length,
            truncation=True,
            padding="max_length",
            add_special_tokens=True
        )
        tokenized["labels"] = batch["label"]
        return tokenized

    return dataset.map(tokenize_fn, batched=True, num_proc=4)


def fine_tune_model(
    train_data_path: str,
    val_data_path: str,
    model_save_path: str,
    output_dir: str = "results/voicephishing",
    log_dir: str = "logs/voicephishing",
    model_name: str = "maywell/Synatra-42dot-1.3B",
    cache_dir: str = "cache",
    num_labels: int = 2,
    learning_rate: float = 2e-5,
    num_train_epochs: int = 5,
    per_device_train_batch_size: int = 16,
    max_length: int = 1024,
    lora_r: int = 16,
    lora_alpha: int = 32,
    lora_dropout: float = 0.2,
    hf_token: str = ""
):
    """
    ëª¨ë¸ íŒŒì¸íŠœë‹ ìˆ˜í–‰
    
    Args:
        train_data_path: í•™ìŠµ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        val_data_path: ê²€ì¦ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        model_save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        model_name: ëª¨ë¸ ì´ë¦„
        cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)
        num_labels: ë¶„ë¥˜ ë ˆì´ë¸” ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        num_train_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
        per_device_train_batch_size: ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°
        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        lora_r: LoRA rank
        lora_alpha: LoRA alpha
        lora_dropout: LoRA dropout
        hf_token: Hugging Face í† í°
    """
    # ì‹œë“œ ê³ ì •
    set_seed(42)
    
    # í™˜ê²½ ì„¤ì •
    cache_path = setup_environment(cache_dir=cache_dir, hf_token=hf_token)
    
    # ê²½ë¡œ ì„¤ì • (ìƒëŒ€ê²½ë¡œ)
    base_path = Path(__file__).parent.parent
    train_path = base_path / train_data_path
    val_path = base_path / val_data_path
    model_path = base_path / model_save_path
    output_path = base_path / output_dir
    log_path = base_path / log_dir
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    model_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.mkdir(parents=True, exist_ok=True)
    log_path.mkdir(parents=True, exist_ok=True)
    
    print(f"âœ… í•™ìŠµ ë°ì´í„° ë¡œë“œ: {train_path}")
    print(f"âœ… ê²€ì¦ ë°ì´í„° ë¡œë“œ: {val_path}")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    train_dataset = load_dataset("csv", data_files=str(train_path))["train"]
    val_dataset = load_dataset("csv", data_files=str(val_path))["train"]
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    print("ğŸ“¦ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    model, tokenizer = load_model_and_tokenizer(
        model_name=model_name,
        cache_dir=cache_dir,
        num_labels=num_labels,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout
    )
    
    # ë°ì´í„° í† í°í™”
    print("ğŸ”„ ë°ì´í„° í† í°í™” ì¤‘...")
    tokenized_train = tokenize_voicephishing_data(train_dataset, tokenizer, max_length)
    tokenized_val = tokenize_voicephishing_data(val_dataset, tokenizer, max_length)
    
    # í•™ìŠµ ì¸ì ì„¤ì •
    training_args = TrainingArguments(
        output_dir=str(output_path),
        learning_rate=learning_rate,
        lr_scheduler_type="cosine",
        warmup_ratio=0.2,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_train_batch_size,
        save_steps=100,
        save_total_limit=3,
        logging_dir=str(log_path),
        logging_steps=100,
        optim="paged_adamw_32bit",
        max_grad_norm=5,
        eval_strategy="steps",
        eval_steps=100,
        load_best_model_at_end=True,
        ddp_find_unused_parameters=False,
        bf16=True
    )
    
    # Trainer ì„¤ì •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_val,
        data_collator=DataCollatorWithPadding(tokenizer),
        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
    )
    
    # í•™ìŠµ ì‹œì‘
    print("ğŸš€ í•™ìŠµ ì‹œì‘...")
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {model_path}")
    model.save_pretrained(str(model_path))
    
    print("âœ… í•™ìŠµ ì™„ë£Œ!")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ë³´ì´ìŠ¤í”¼ì‹± íƒì§€ ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--train_data", type=str, default="dataset/bt_all_512_25.csv",
                        help="í•™ìŠµ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--val_data", type=str, default="dataset/total_dataset_val.csv",
                        help="ê²€ì¦ ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--model_save", type=str, default="model/model_synatra_bt_all_512_25",
                        help="ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--output_dir", type=str, default="results/voicephishing",
                        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--log_dir", type=str, default="logs/voicephishing",
                        help="ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--model_name", type=str, default="maywell/Synatra-42dot-1.3B",
                        help="ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--cache_dir", type=str, default="cache",
                        help="ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ìƒëŒ€ê²½ë¡œ)")
    parser.add_argument("--num_labels", type=int, default=2,
                        help="ë¶„ë¥˜ ë ˆì´ë¸” ìˆ˜")
    parser.add_argument("--learning_rate", type=float, default=2e-5,
                        help="í•™ìŠµë¥ ")
    parser.add_argument("--num_epochs", type=int, default=5,
                        help="í•™ìŠµ ì—í¬í¬ ìˆ˜")
    parser.add_argument("--batch_size", type=int, default=16,
                        help="ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--max_length", type=int, default=1024,
                        help="ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´")
    parser.add_argument("--lora_r", type=int, default=16,
                        help="LoRA rank")
    parser.add_argument("--lora_alpha", type=int, default=32,
                        help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.2,
                        help="LoRA dropout")
    parser.add_argument("--hf_token", type=str, default="",
                        help="Hugging Face í† í° (í™˜ê²½ë³€ìˆ˜ HUGGINGFACE_TOKEN ì‚¬ìš© ê°€ëŠ¥)")
    
    args = parser.parse_args()
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ í† í° ê°€ì ¸ì˜¤ê¸°
    hf_token = args.hf_token or os.getenv("HUGGINGFACE_TOKEN", "")
    
    fine_tune_model(
        train_data_path=args.train_data,
        val_data_path=args.val_data,
        model_save_path=args.model_save,
        output_dir=args.output_dir,
        log_dir=args.log_dir,
        model_name=args.model_name,
        cache_dir=args.cache_dir,
        num_labels=args.num_labels,
        learning_rate=args.learning_rate,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        max_length=args.max_length,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        hf_token=hf_token
    )

